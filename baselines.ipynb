{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "830b1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae52ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, X_train, y_train, X_val, y_val, X_test, eps=1e-15):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    proba_val = model.predict_proba(X_val)\n",
    "    \n",
    "    # clip and normalize for numerical stability\n",
    "    proba_val = np.clip(proba_val, eps, 1 - eps)\n",
    "    proba_val = proba_val / proba_val.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # calculate loss - use model.classes_ to match probability order\n",
    "    loss = log_loss(y_val, proba_val, labels=model.classes_)\n",
    "    \n",
    "    proba_test = model.predict_proba(X_test)\n",
    "    proba_test = np.clip(proba_test, eps, 1 - eps)\n",
    "    proba_test = proba_test / proba_test.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    return loss, proba_val, proba_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c93644",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "meta = json.load(open(\"./dataset/meta.json\"))\n",
    "test = pd.read_csv(\"./dataset/test_processed.csv\")\n",
    "train = pd.read_csv(\"./dataset/train_processed.csv\")\n",
    "\n",
    "features = meta[\"features\"]\n",
    "num_classes = meta[\"n_classes\"]\n",
    "class_names = meta[\"class_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b95ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(744367, 124), X_val shape:(131359, 124), X_test shape:(884262, 124)\n",
      "num of classes: 39\n",
      "['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY', 'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE', 'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION', 'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING', 'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING', 'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES', 'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE', 'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE', 'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE', 'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT', 'WARRANTS', 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "y = train[\"target\"].values\n",
    "X, X_test = train[features], test[features]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "\n",
    "num_val_class = len(np.unique(y_val))\n",
    "num_train_class = len(np.unique(y_train))\n",
    "print(f\"X_train shape:{X_train.shape}, X_val shape:{X_val.shape}, X_test shape:{X_test.shape}\")\n",
    "\n",
    "assert num_val_class == num_classes\n",
    "assert num_val_class == num_train_class\n",
    "print(f\"num of classes: {num_train_class}\")\n",
    "print(class_names)\n",
    "\n",
    "results, val_store, test_store = {}, {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551287b",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a3e3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(744367, 24), X_val shape:(131359, 24), X_test shape:(884262, 24)\n",
      "num of classes: 39\n",
      "['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY', 'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE', 'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION', 'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING', 'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING', 'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES', 'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE', 'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE', 'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE', 'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT', 'WARRANTS', 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "rf_features =  [\"X\", \"Y\",\"X+Y\",\"X-Y\",\"XY_rad\",\"XYpca1\",\"XYpca2\",\"Hour\",\"Month\",\"Year\",\"Day\",\"Minute\",\"StreetName_freq\",\"AddrType_freq\",\n",
    "                \"PdDistrict_freq\",\"DayOfWeek_freq\",\"GeoCluster_freq\",\"Night\",\"Is_weekend\",\"Intersection\",\"DayOfWeek_le\",\"PdDistrict_le\",\"AddrType_le\", \"GeoCluster_le\"]\n",
    "\n",
    "y = train[\"target\"].values\n",
    "X, X_test = train[rf_features], test[rf_features]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "\n",
    "num_val_class = len(np.unique(y_val))\n",
    "num_train_class = len(np.unique(y_train))\n",
    "print(f\"X_train shape:{X_train.shape}, X_val shape:{X_val.shape}, X_test shape:{X_test.shape}\")\n",
    "\n",
    "assert num_val_class == num_classes\n",
    "assert num_val_class == num_train_class\n",
    "print(f\"num of classes: {num_train_class}\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8735822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training random forest...\n",
      "random forest validation log-loss: 3.151723650550059\n"
     ]
    }
   ],
   "source": [
    "print(\"training random forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=SEED)\n",
    "rf_loss, rf_val, rf_test = train_and_eval(rf, X_train, y_train, X_val, y_val, X_test)\n",
    "results['RandomForest'] = rf_loss\n",
    "val_store['RandomForest'] = rf_val\n",
    "test_store['RandomForest'] = rf_test\n",
    "print(\"random forest validation log-loss:\", rf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2f2b7c",
   "metadata": {},
   "source": [
    "### Histogram-based Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2423a973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training HistGradientBoostingClassifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abidhassan/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2916: UserWarning: The y_pred values do not sum to one. Starting from 1.5 thiswill result in an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGB validation log-loss: 2.528305126608513\n"
     ]
    }
   ],
   "source": [
    "print(\"training HistGradientBoostingClassifier...\")\n",
    "hgb = HistGradientBoostingClassifier( max_iter=500, random_state=SEED)\n",
    "hgb_loss, hgb_val, hgb_test = train_and_eval(hgb, X_train, y_train, X_val, y_val, X_test)\n",
    "results['HistGB'] = hgb_loss\n",
    "val_store['HistGB'] = hgb_val\n",
    "test_store['HistGB'] = hgb_test\n",
    "print(\"HistGB validation log-loss:\", hgb_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12e513",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2854eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training adaBoost...\n",
      "adaBoost validation log-loss: 3.6228212586949313\n"
     ]
    }
   ],
   "source": [
    "print(\"training adaBoost...\")\n",
    "adb = AdaBoostClassifier(n_estimators=200, random_state=SEED)\n",
    "adb_loss, adb_val, adb_test = train_and_eval(adb, X_train, y_train, X_val, y_val, X_test)\n",
    "results['AdaBoost'] = adb_loss\n",
    "val_store['AdaBoost'] = adb_val\n",
    "test_store['AdaBoost'] = adb_test\n",
    "print(\"adaBoost validation log-loss:\", adb_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538544a1",
   "metadata": {},
   "source": [
    "### Multinomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4131bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:(744367, 124), X_val shape:(131359, 124), X_test shape:(884262, 124)\n",
      "num of classes: 39\n",
      "['ARSON', 'ASSAULT', 'BAD CHECKS', 'BRIBERY', 'BURGLARY', 'DISORDERLY CONDUCT', 'DRIVING UNDER THE INFLUENCE', 'DRUG/NARCOTIC', 'DRUNKENNESS', 'EMBEZZLEMENT', 'EXTORTION', 'FAMILY OFFENSES', 'FORGERY/COUNTERFEITING', 'FRAUD', 'GAMBLING', 'KIDNAPPING', 'LARCENY/THEFT', 'LIQUOR LAWS', 'LOITERING', 'MISSING PERSON', 'NON-CRIMINAL', 'OTHER OFFENSES', 'PORNOGRAPHY/OBSCENE MAT', 'PROSTITUTION', 'RECOVERED VEHICLE', 'ROBBERY', 'RUNAWAY', 'SECONDARY CODES', 'SEX OFFENSES FORCIBLE', 'SEX OFFENSES NON FORCIBLE', 'STOLEN PROPERTY', 'SUICIDE', 'SUSPICIOUS OCC', 'TREA', 'TRESPASS', 'VANDALISM', 'VEHICLE THEFT', 'WARRANTS', 'WEAPON LAWS']\n"
     ]
    }
   ],
   "source": [
    "y = train[\"target\"].values\n",
    "X, X_test = train[features], test[features]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "\n",
    "num_val_class = len(np.unique(y_val))\n",
    "num_train_class = len(np.unique(y_train))\n",
    "print(f\"X_train shape:{X_train.shape}, X_val shape:{X_val.shape}, X_test shape:{X_test.shape}\")\n",
    "\n",
    "assert num_val_class == num_classes\n",
    "assert num_val_class == num_train_class\n",
    "print(f\"num of classes: {num_train_class}\")\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35dd5c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training multi-logistic regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abidhassan/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression validation log-loss: 2.4444801511664873\n"
     ]
    }
   ],
   "source": [
    "print(\"training multi-logistic regression...\")\n",
    "lr = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", C=5.0, max_iter=300, random_state=SEED, n_jobs=-1)\n",
    "lr_loss, lr_val, lr_test = train_and_eval(lr, X_train, y_train, X_val, y_val, X_test)\n",
    "results['LogisticRegression'] = lr_loss\n",
    "val_store['LogisticRegression'] = lr_val\n",
    "test_store['LogisticRegression'] = lr_test\n",
    "print(\"logistic regression validation log-loss:\", lr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4da57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(lr, \"./checkpoints/logistic_regression.pkl\")\n",
    "lr_loaded = joblib.load(\"./checkpoints/logistic_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a698bcf",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73fed25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model validation log-loss:\n",
      "  LogisticRegression   : 2.44448\n",
      "  HistGB               : 2.52831\n",
      "  RandomForest         : 3.15172\n",
      "  AdaBoost             : 3.62282\n",
      "top 4 models for soft-average ensemble: ['LogisticRegression', 'HistGB', 'RandomForest', 'AdaBoost']\n",
      "soft-average validation log-loss: 2.392075591988043\n",
      "best single model: LogisticRegression loss: 2.4444801511664873\n"
     ]
    }
   ],
   "source": [
    "print(\"model validation log-loss:\")\n",
    "for k, v in sorted(results.items(), key=lambda kv: kv[1]):\n",
    "    print(f\"  {k:20s} : {v:.5f}\")\n",
    "\n",
    "# soft average of top 4 models\n",
    "ordered = sorted(results.items(), key=lambda kv: kv[1])\n",
    "top_models = [k for k, _ in ordered[:4]]\n",
    "print(\"top 4 models for soft-average ensemble:\", top_models)\n",
    "\n",
    "val_avg = np.zeros_like(val_store[top_models[0]])\n",
    "test_avg = np.zeros_like(test_store[top_models[0]])\n",
    "\n",
    "for m in top_models:\n",
    "    val_avg += val_store[m] / len(top_models)\n",
    "    test_avg += test_store[m] / len(top_models)\n",
    "\n",
    "avg_loss = log_loss(y_val, val_avg)\n",
    "print(\"soft-average validation log-loss:\", avg_loss)\n",
    "\n",
    "# save best single and soft-average submission\n",
    "best_single = min(results.items(), key=lambda kv: kv[1])[0]\n",
    "print(\"best single model:\", best_single, \"loss:\", results[best_single])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64ce44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save submissions\n",
    "class_cols = class_names\n",
    "\n",
    "# best single model\n",
    "submission_single = pd.DataFrame(test_store[best_single], columns=class_cols)\n",
    "submission_single[\"Id\"] = test.index if \"Id\" not in test.columns else test[\"Id\"].astype(int).values\n",
    "submission_single = submission_single[[\"Id\"] + class_cols]\n",
    "submission_single.to_csv(f\"./submissions/{best_single}_submission.csv\", index=False)\n",
    "\n",
    "# Soft-Average\n",
    "submission_avg = pd.DataFrame(test_avg, columns=class_cols)\n",
    "submission_avg[\"Id\"] = test.index if \"Id\" not in test.columns else test[\"Id\"].astype(int).values\n",
    "submission_avg = submission_avg[[\"Id\"] + class_cols]\n",
    "submission_avg.to_csv(\"./submissions/soft_average_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
