{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0f5e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import gensim\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5b213",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b245b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(train_df, test_df, drop_cols = ['Descript', 'Resolution']):\n",
    "    for col in drop_cols:\n",
    "        if col in test_df.columns: test_df.drop(col, axis=1, inplace=True)\n",
    "        if col in train_df.columns: train_df.drop(col, axis=1, inplace=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "def basic_time_features(df):\n",
    "    df[\"Dates\"] = pd.to_datetime(df[\"Dates\"], infer_datetime_format=True)    \n",
    "    df[\"Day\"] = df[\"Dates\"].dt.day\n",
    "    df[\"Hour\"] = df[\"Dates\"].dt.hour\n",
    "    df[\"Year\"] = df[\"Dates\"].dt.year\n",
    "    df[\"Month\"] = df[\"Dates\"].dt.month\n",
    "    df[\"Minute\"] = df[\"Dates\"].dt.minute\n",
    "    df[\"DayOfWeek\"] = df[\"DayOfWeek\"].astype(str)\n",
    "    df['Night'] = df['Hour'].apply(lambda x: 1 if x > 6 and x < 18 else 0)\n",
    "    df[\"Is_weekend\"] = df[\"DayOfWeek\"].isin([\"Saturday\", \"Sunday\"]).astype(int)\n",
    "    \n",
    "    df.drop('Dates', axis=1, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2da706d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_coord_outliers(train_df, test_df):\n",
    "    # removes outlier datapoints like north pole or out of SF area. SF bounding box: lon [-123, -121], lat [37, 38]\n",
    "    test_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "    train_df.replace({'X': -120.5, 'Y': 90.0}, np.NaN, inplace=True)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    for district in train_df['PdDistrict'].unique():\n",
    "        train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']] = imputer.fit_transform(train_df.loc[train_df['PdDistrict'] == district, ['X', 'Y']])\n",
    "        test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']] = imputer.transform(test_df.loc[test_df['PdDistrict'] == district, ['X', 'Y']])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_geo_clusters(train, test, num_clusters=150):\n",
    "\n",
    "    test[\"X+Y\"], test[\"X-Y\"] = test[\"X\"] + test[\"Y\"], test[\"X\"] - test[\"Y\"]\n",
    "    train[\"X+Y\"], train[\"X-Y\"] = train[\"X\"] + train[\"Y\"], train[\"X\"] - train[\"Y\"]\n",
    "\n",
    "    test[\"XY_rad\"] = np.sqrt(np.power(test['Y'], 2) + np.power(test['X'], 2))\n",
    "    train[\"XY_rad\"] = np.sqrt(np.power(train['Y'], 2) + np.power(train['X'], 2))\n",
    "\n",
    "    # combine all coordinates\n",
    "    coords = np.vstack([\n",
    "        train[[\"X\", \"Y\"]].values,\n",
    "        test[[\"X\", \"Y\"]].values,\n",
    "    ])\n",
    "    \n",
    "    # impute missing values with median coordinates (better than 0)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    coords_imputed = imputer.fit_transform(coords)\n",
    "    \n",
    "    # impute and predict for both datasets\n",
    "    test_coords_imputed = imputer.transform(test[[\"X\", \"Y\"]].values)\n",
    "    train_coords_imputed = imputer.transform(train[[\"X\", \"Y\"]].values)\n",
    "\n",
    "\n",
    "    # Adding PCA component\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2).fit(coords_imputed)\n",
    "    test_pca = pca.transform(test_coords_imputed)\n",
    "    train_pca = pca.transform(train_coords_imputed)\n",
    "    test[\"XYpca1\"], test[\"XYpca2\"] = test_pca[:, 0], test_pca[:, 1]\n",
    "    train[\"XYpca1\"], train[\"XYpca2\"] = train_pca[:, 0], train_pca[:, 1] \n",
    "    \n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    clf = GaussianMixture(n_components=num_clusters, covariance_type=\"diag\", random_state=0).fit(coords_imputed)\n",
    "    test[\"GeoCluster\"] = clf.predict(test_coords_imputed)\n",
    "    train[\"GeoCluster\"] = clf.predict(train_coords_imputed)\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e932726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing address: street, block, intersection\n",
    "def parse_address(addr):\n",
    "    if pd.isna(addr):\n",
    "        return \"UNKNOWN\", \"UNKNOWN\"\n",
    "    if \"/\" in addr:\n",
    "        parts = [p.strip() for p in addr.split(\"/\")]\n",
    "        return \"INTERSECTION\", \"_\".join(sorted(parts))\n",
    "    m = re.search(r\"(\\d+)\\s+Block\\s+of\\s+(.+)\", addr, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return \"BLOCK\", m.group(2).strip()\n",
    "    return \"STREET\", addr.strip()\n",
    "\n",
    "# address embeddings\n",
    "def address_encoding(train, test):\n",
    "    # add intersection binary feature\n",
    "    test['Intersection'] = test['Address'].apply(lambda x: 1 if '/' in x else 0)\n",
    "    train['Intersection'] = train['Address'].apply(lambda x: 1 if '/' in x else 0)\n",
    "    print(f\"after intersection, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "\n",
    "    train_length = len(train)\n",
    "\n",
    "    combined = pd.concat([train, test], ignore_index=True)\n",
    "    address_list = [address.split(' ') for address in combined['Address']]\n",
    "    address_model = gensim.models.Word2Vec(address_list, min_count=1)\n",
    "\n",
    "    address_embeddings = np.zeros((combined.shape[0], 100))\n",
    "    for i in range(len(address_list)):\n",
    "        for j in range(len(address_list[i])):\n",
    "            address_embeddings[i] += address_model.wv[address_list[i][j]]\n",
    "        address_embeddings[i] /= len(address_list[i])\n",
    "\n",
    "    encoding_cols = []\n",
    "    for i in range(address_embeddings.shape[1]):\n",
    "        encoding_cols.append(\"EncodedAddress{}\".format(i))\n",
    "    \n",
    "    encoding_address_df = pd.DataFrame(address_embeddings, columns=encoding_cols)\n",
    "    combined = pd.concat([combined, encoding_address_df], axis=1, sort=False)\n",
    "    combined.drop('Address', axis=1, inplace=True)\n",
    "\n",
    "    train = combined[:train_length]\n",
    "    test = combined[train_length:]\n",
    "    \n",
    "    return train, test, encoding_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f55c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_freq_encodings(train, test, cols):\n",
    "    for c in cols:\n",
    "        vc = train[c].fillna(\"__NA__\").value_counts(dropna=False)\n",
    "        train[f\"{c}_freq\"] = train[c].fillna(\"__NA__\").map(vc).astype(int)\n",
    "        test[f\"{c}_freq\"] = test[c].fillna(\"__NA__\").map(vc).fillna(0).astype(int)\n",
    "\n",
    "\n",
    "def fit_label_encoders(train, test, cat_cols):\n",
    "    encoders = {}\n",
    "    for c in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(pd.concat([train[c].astype(str), test[c].astype(str)], axis=0).astype(str))\n",
    "        train[c + \"_le\"] = le.transform(train[c].astype(str))\n",
    "        test[c + \"_le\"] = le.transform(test[c].astype(str))\n",
    "        encoders[c] = le\n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521d0587",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873e9ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset in csv format ...\n",
      "train shape: (875726, 9), test shape: (884262, 7)\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "num_clusters = 150\n",
    "out_dir = \"./dataset/\"\n",
    "test = \"./dataset/test.csv\"\n",
    "train = \"./dataset/train.csv\"\n",
    "\n",
    "print(\"loading dataset in csv format ...\")\n",
    "test = pd.read_csv(test, parse_dates=[\"Dates\"])   # expects Kaggle format\n",
    "train = pd.read_csv(train, parse_dates=[\"Dates\"])  # expects Kaggle format\n",
    "\n",
    "# drop duplicates\n",
    "train.drop_duplicates(inplace=True)\n",
    "\n",
    "debug = False\n",
    "if debug:\n",
    "    train = train.sample(8000, random_state=SEED).reset_index(drop=True)\n",
    "    test = test.sample(2000, random_state=SEED).reset_index(drop=True)\n",
    "print(f\"train shape: {train.shape}, test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ae5ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after basic cleaning, train shape: (875726, 9), test shape: (884262, 7)\n",
      "after time feature engineering, train shape: (875726, 15), test shape: (884262, 13)\n",
      "after dropping columns, train shape: (875726, 13), test shape: (884262, 13)\n"
     ]
    }
   ],
   "source": [
    "# basic cleaning\n",
    "train, test = remove_coord_outliers(train, test)\n",
    "print(f\"after basic cleaning, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "# time features\n",
    "test = basic_time_features(test)\n",
    "train = basic_time_features(train)\n",
    "print(f\"after time feature engineering, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "# drop columns: ['Descript', 'Resolution', 'Id']\n",
    "train, test = drop_columns(train, test, drop_cols = ['Descript', 'Resolution'])\n",
    "print(f\"after dropping columns, train shape: {train.shape}, test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c98f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after address parsing, train shape: (875726, 15), test shape: (884262, 15)\n",
      "after geo-clustering, train shape: (875726, 21), test shape: (884262, 21)\n",
      "after frequencry encoding, train shape: (875726, 26), test shape: (884262, 26)\n",
      "after intersection, train shape: (875726, 27), test shape: (884262, 27)\n",
      "after address encoding, train shape: (875726, 127), test shape: (884262, 127)\n"
     ]
    }
   ],
   "source": [
    "# address parsing\n",
    "for df in (train, test):\n",
    "    df[[\"AddrType\", \"StreetName\"]] = df[\"Address\"].apply(lambda x: pd.Series(parse_address(x)))\n",
    "print(f\"after address parsing, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "# geo clustering (its not that useful to do)\n",
    "train, test = add_geo_clusters(train, test, num_clusters=num_clusters)\n",
    "print(f\"after geo-clustering, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "# frequency encodings\n",
    "add_freq_encodings(train, test, [\"StreetName\", \"AddrType\", \"PdDistrict\", \"DayOfWeek\", \"GeoCluster\"])\n",
    "print(f\"after frequencry encoding, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "\n",
    "# address embeddings\n",
    "train, test, encoding_cols = address_encoding(train, test)\n",
    "print(f\"after address encoding, train shape: {train.shape}, test shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d15b7a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after processing, train shape: (875726, 132), test shape: (884262, 131)\n"
     ]
    }
   ],
   "source": [
    "# label encoders\n",
    "cat_cols = [\"DayOfWeek\", \"PdDistrict\", \"AddrType\", \"GeoCluster\"]\n",
    "encoders = fit_label_encoders(train, test, cat_cols)\n",
    "\n",
    "# numeric features and fill missing\n",
    "bin_cols = [\"Night\", \"Is_weekend\", \"Intersection\"]\n",
    "num_cols = [\"X\", \"Y\", \"X+Y\", \"X-Y\", \"XY_rad\", \"XYpca1\", \"XYpca2\", \"Hour\", \"Month\", \"Year\", \"Day\", \n",
    "            \"Minute\", \"StreetName_freq\", \"AddrType_freq\", \"PdDistrict_freq\", \"DayOfWeek_freq\", \"GeoCluster_freq\"]\n",
    "\n",
    "for c in num_cols:\n",
    "    if c not in train.columns:\n",
    "        train[c] = 0\n",
    "        test[c] = 0\n",
    "    train[c] = train[c].fillna(-999)\n",
    "    test[c] = test[c].fillna(-999)\n",
    "\n",
    "# scale numerics\n",
    "scaler = StandardScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "\n",
    "\n",
    "# target encoding label\n",
    "target_col = \"Category\"\n",
    "le_target = LabelEncoder()\n",
    "train[\"target\"] = le_target.fit_transform(train[target_col].astype(str))\n",
    "\n",
    "print(f\"after processing, train shape: {train.shape}, test shape: {test.shape}\")\n",
    "feature_list = num_cols + bin_cols + encoding_cols + [c + \"_le\" for c in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acf87368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving processed data to ./dataset/\n",
      "processing done and files saved :\n",
      " - ./dataset/meta.json\n",
      " - ./dataset/test_processed.csv\n",
      " - ./dataset/train_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# save processed files\n",
    "meta = {\n",
    "    \"bin_cols\": bin_cols,\n",
    "    \"num_cols\": num_cols,\n",
    "    \"cat_cols\": cat_cols,\n",
    "    \"cat_cols_le\": [c + \"_le\" for c in cat_cols],\n",
    "    \"encodings\" : encoding_cols,\n",
    "    \"features\": feature_list,\n",
    "    \"n_classes\": int(train[\"target\"].nunique()),\n",
    "    \"class_names\": list(le_target.classes_),\n",
    "    \"gaussian_n_clusters\": int(num_clusters)\n",
    "}\n",
    "\n",
    "meta_out = \"./dataset/meta.json\"\n",
    "test_out = \"./dataset/test_processed.csv\"\n",
    "train_out = \"./dataset/train_processed.csv\"\n",
    "\n",
    "\n",
    "print(\"saving processed data to\", out_dir)\n",
    "train.to_csv(train_out, index=False)\n",
    "test.to_csv(test_out, index=False)\n",
    "with open(meta_out, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"processing done and files saved :\")\n",
    "for p in [meta_out, test_out,train_out ]:\n",
    "    print(\" -\", p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
